{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#scale features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from pyod.models.ecod import ECOD\n",
    "from pyod.models.copod import COPOD\n",
    "from pyod.models.iforest import IForest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "def assign_labels(features1, features2, radius=2.0):\n",
    "    \"\"\"\n",
    "    Assign labels to features1 from features2 where\n",
    "    1) frame_id matches and\n",
    "    2) distance between (x, y) <= radius.\n",
    "    \n",
    "    Returns an updated copy of features1 with a 'label' column.\n",
    "    \"\"\"\n",
    "    # Ensure we have a 'label' column in features1, set default 0\n",
    "    if 'label' not in features1.columns:\n",
    "        features1['label'] = 0\n",
    "    \n",
    "    # Convert to avoid SettingWithCopyWarnings:\n",
    "    features1 = features1.copy()\n",
    "    features2 = features2.copy()\n",
    "    \n",
    "    # We will store subsets of features1 in a dictionary keyed by frame_id\n",
    "    grouped_f1 = dict(tuple(features1.groupby('frame_id')))\n",
    "    # Also group features2 by frame_id\n",
    "    grouped_f2 = dict(tuple(features2.groupby('frame_id')))\n",
    "    \n",
    "    # For efficient column access:\n",
    "    label_col_idx = features1.columns.get_loc('label')\n",
    "    \n",
    "    # We'll also store updated subsets in a dictionary\n",
    "    updated_subsets = {}\n",
    "    \n",
    "    # Iterate only over frame_ids that are in features1\n",
    "    for fid, subset_f1 in grouped_f1.items():\n",
    "        # Build a KDTree for the (x, y) coords in features1 for this frame\n",
    "        coords_f1 = subset_f1[['x', 'y']].values\n",
    "        if len(coords_f1) > 0:\n",
    "            kdtree = KDTree(coords_f1)\n",
    "        else:\n",
    "            updated_subsets[fid] = subset_f1\n",
    "            continue\n",
    "        \n",
    "        # If this frame_id also exists in features2, we do the radius queries\n",
    "        if fid in grouped_f2:\n",
    "            subset_f2 = grouped_f2[fid]\n",
    "            coords_f2 = subset_f2[['x', 'y']].values\n",
    "            \n",
    "            # Use query_radius to find neighbors within the given radius\n",
    "            neighbor_indices_array = kdtree.query_radius(coords_f2, r=radius)\n",
    "            \n",
    "            # Convert subset_f1 to numpy for assignment, then we will put it back in a DataFrame\n",
    "            subset_f1_values = subset_f1.values\n",
    "            \n",
    "            # Assign label for each set of neighbor indices\n",
    "            for i, indices in enumerate(neighbor_indices_array):\n",
    "                if len(indices) > 0:\n",
    "                    lbl = subset_f2.iloc[i]['label']\n",
    "                    subset_f1_values[indices, label_col_idx] = lbl\n",
    "            \n",
    "            # Re-wrap in a dataframe\n",
    "            subset_f1_updated = pd.DataFrame(subset_f1_values, \n",
    "                                             columns=subset_f1.columns,\n",
    "                                             index=subset_f1.index)\n",
    "        else:\n",
    "            # No labels to assign if there's no corresponding frame in features2\n",
    "            subset_f1_updated = subset_f1\n",
    "        \n",
    "        updated_subsets[fid] = subset_f1_updated\n",
    "    \n",
    "    # Concatenate all updated subsets back together\n",
    "    updated_features1 = pd.concat(updated_subsets.values(), axis=0)\n",
    "    \n",
    "    return updated_features1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0B68620\n",
      "Scaling Features\n",
      "Running COPOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:   11.8s remaining:  6.1min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   20.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPOD Completed, Storing results\n",
      "Running ECOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    2.2s remaining:  1.1min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:    9.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECOD Completed, Storing results\n",
      "Running IForest on features\n",
      "IForest Completed, Storing results\n",
      "Processing 0B60414\n",
      "Scaling Features\n",
      "Running COPOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    3.0s remaining:  1.6min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   17.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPOD Completed, Storing results\n",
      "Running ECOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    2.5s remaining:  1.3min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   17.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECOD Completed, Storing results\n",
      "Running IForest on features\n",
      "IForest Completed, Storing results\n",
      "Processing 0B87816\n",
      "Scaling Features\n",
      "Running COPOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    3.0s remaining:  1.6min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   17.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPOD Completed, Storing results\n",
      "Running ECOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    2.9s remaining:  1.5min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   17.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECOD Completed, Storing results\n",
      "Running IForest on features\n",
      "IForest Completed, Storing results\n",
      "Processing 0B87911\n",
      "Scaling Features\n",
      "Running COPOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    2.7s remaining:  1.4min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   16.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPOD Completed, Storing results\n",
      "Running ECOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    3.3s remaining:  1.7min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   16.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECOD Completed, Storing results\n",
      "Running IForest on features\n",
      "IForest Completed, Storing results\n",
      "Processing 0B68720\n",
      "Scaling Features\n",
      "Running COPOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    2.6s remaining:  1.3min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   15.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPOD Completed, Storing results\n",
      "Running ECOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    3.7s remaining:  1.9min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   15.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECOD Completed, Storing results\n",
      "Running IForest on features\n",
      "IForest Completed, Storing results\n",
      "Processing 0B8B306\n",
      "Scaling Features\n",
      "Running COPOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    2.6s remaining:  1.3min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   18.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPOD Completed, Storing results\n",
      "Running ECOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    3.0s remaining:  1.5min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   18.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECOD Completed, Storing results\n",
      "Running IForest on features\n",
      "IForest Completed, Storing results\n",
      "Processing 0B68520\n",
      "Scaling Features\n",
      "Running COPOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    3.2s remaining:  1.7min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   15.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPOD Completed, Storing results\n",
      "Running ECOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    3.1s remaining:  1.6min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   15.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECOD Completed, Storing results\n",
      "Running IForest on features\n",
      "IForest Completed, Storing results\n",
      "Processing 0B60214\n",
      "Scaling Features\n",
      "Running COPOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    3.0s remaining:  1.5min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   17.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPOD Completed, Storing results\n",
      "Running ECOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    2.5s remaining:  1.3min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   17.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECOD Completed, Storing results\n",
      "Running IForest on features\n",
      "IForest Completed, Storing results\n",
      "Processing 0B60314\n",
      "Scaling Features\n",
      "Running COPOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    2.8s remaining:  1.5min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   16.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPOD Completed, Storing results\n",
      "Running ECOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    2.8s remaining:  1.4min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   16.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECOD Completed, Storing results\n",
      "Running IForest on features\n",
      "IForest Completed, Storing results\n",
      "Processing 0B87711\n",
      "Scaling Features\n",
      "Running COPOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    2.9s remaining:  1.5min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   20.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPOD Completed, Storing results\n",
      "Running ECOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    3.2s remaining:  1.7min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   21.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECOD Completed, Storing results\n",
      "Running IForest on features\n",
      "IForest Completed, Storing results\n",
      "Processing 0B60114\n",
      "Scaling Features\n",
      "Running COPOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    2.6s remaining:  1.4min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   11.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPOD Completed, Storing results\n",
      "Running ECOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    2.6s remaining:  1.4min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   11.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECOD Completed, Storing results\n",
      "Running IForest on features\n",
      "IForest Completed, Storing results\n",
      "Processing 0B68820\n",
      "Scaling Features\n",
      "Running COPOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    3.0s remaining:  1.6min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   16.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPOD Completed, Storing results\n",
      "Running ECOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    3.0s remaining:  1.6min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   15.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECOD Completed, Storing results\n",
      "Running IForest on features\n",
      "IForest Completed, Storing results\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_path = '/mnt/deepstore/Final_DeepPhenotyping/pipeline/output/spike_0428'\n",
    "annotated_path = '/mnt/deepstore/Final_DeepPhenotyping/figures/figure5_spikein/annotated_slides'\n",
    "slides = os.listdir(data_path)\n",
    "slides = [s for s in slides if os.path.isdir(os.path.join(data_path, s))]\n",
    "\n",
    "#make a dictionary to store the results for each slide\n",
    "results = {}\n",
    "\n",
    "\n",
    "for slide in slides: \n",
    "\n",
    "    print(f'Processing {slide}')\n",
    "\n",
    "    features1 = pd.read_parquet(os.path.join(data_path, slide, f'{slide}.parquet.gz'))\n",
    "    features2 = pd.read_hdf(os.path.join(annotated_path, slide, f'{slide}.hdf5'), key='features')\n",
    "\n",
    "    features1 = assign_labels(features1, features2, radius=3.0)\n",
    "\n",
    "    #features1 contains the features, 'label' contains the labels\n",
    "    del features2 #free memory\n",
    "\n",
    "    #run the model on features1 columns 'z0' to 'z127' and store the output in 'prediction'\n",
    "\n",
    "    features1 = features1.dropna()\n",
    "    features1 = features1.reset_index(drop=True)\n",
    "\n",
    "    features = features1.loc[:, 'z0':'z127'].values\n",
    "\n",
    "    print(\"Scaling Features\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    features = scaler.fit_transform(features)\n",
    "    \n",
    "    contamination = 1e-3\n",
    "\n",
    "    print(\"Running COPOD on features\")\n",
    "\n",
    "    clf_name = 'COPOD'\n",
    "    clf = COPOD(n_jobs=-1, contamination=contamination)\n",
    "    clf.fit(features)\n",
    "\n",
    "    print(\"COPOD Completed, Storing results\")\n",
    "\n",
    "    features1['copod_scores'] = clf.decision_scores_\n",
    "\n",
    "    print(\"Running ECOD on features\")\n",
    "    clf_name = 'ECOD'\n",
    "    clf = ECOD(contamination=contamination,n_jobs=-1)\n",
    "    clf.fit(features)\n",
    "    print(\"ECOD Completed, Storing results\")\n",
    "    features1['ecod_scores'] = clf.decision_scores_\n",
    "\n",
    "    print(\"Running IForest on features\")\n",
    "    clf_name = 'IForest'\n",
    "    clf = IForest(contamination=contamination, n_jobs=-1, n_estimators=100)\n",
    "    clf.fit(features)\n",
    "    print(\"IForest Completed, Storing results\")\n",
    "    features1['iforest_scores'] = clf.decision_scores_\n",
    "\n",
    "    #write the features1 to a parquet file\n",
    "    features1 = features1.reset_index(drop=True)\n",
    "    features1.to_parquet(os.path.join(data_path, slide, f'{slide}_OD.parquet.gz'), index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0B68620\n",
      "Results for 0B68620 already exist, skipping\n",
      "Processing 0B60414\n",
      "Results for 0B60414 already exist, skipping\n",
      "Processing 0B87816\n",
      "Results for 0B87816 already exist, skipping\n",
      "Processing 0B87911\n",
      "Results for 0B87911 already exist, skipping\n",
      "Processing 0B68720\n",
      "Results for 0B68720 already exist, skipping\n",
      "Processing 0B8B306\n",
      "Processing 0B68520\n",
      "Processing 0B60214\n",
      "Scaling Features\n",
      "Running COPOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:   18.1s remaining:  9.4min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPOD Completed, Storing results\n",
      "Running ECOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    6.0s remaining:  3.1min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   53.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECOD Completed, Storing results\n",
      "Running IForest on features\n",
      "IForest Completed, Storing results\n",
      "Processing 0B60314\n",
      "Scaling Features\n",
      "Running COPOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    5.6s remaining:  2.9min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   50.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPOD Completed, Storing results\n",
      "Running ECOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    5.6s remaining:  2.9min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   50.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECOD Completed, Storing results\n",
      "Running IForest on features\n",
      "IForest Completed, Storing results\n",
      "Processing 0B87711\n",
      "Scaling Features\n",
      "Running COPOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    7.0s remaining:  3.6min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPOD Completed, Storing results\n",
      "Running ECOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    7.9s remaining:  4.1min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECOD Completed, Storing results\n",
      "Running IForest on features\n",
      "IForest Completed, Storing results\n",
      "Processing 0B60114\n",
      "Processing 0B68820\n",
      "Scaling Features\n",
      "Running COPOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "/home/tessone/miniconda3/envs/prism/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:   14.6s remaining:  7.5min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   59.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPOD Completed, Storing results\n",
      "Running ECOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    5.8s remaining:  3.0min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   49.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECOD Completed, Storing results\n",
      "Running IForest on features\n",
      "IForest Completed, Storing results\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_path = '/mnt/deepstore/Final_DeepPhenotyping/pipeline/output/spike_0428'\n",
    "annotated_path = '/mnt/deepstore/Final_DeepPhenotyping/figures/figure5_outlierdetection/trad_features'\n",
    "slides = os.listdir(data_path)\n",
    "slides = [s for s in slides if os.path.isdir(os.path.join(data_path, s))]\n",
    "\n",
    "#make a dictionary to store the results for each slide\n",
    "results = {}\n",
    "\n",
    "\n",
    "for slide in slides: \n",
    "\n",
    "    print(f'Processing {slide}')\n",
    "\n",
    "    if slide=='0B8B306':\n",
    "        continue\n",
    "    elif slide=='0B60114':\n",
    "        #skip this slide, it is not in the annotated path\n",
    "        continue\n",
    "    elif slide=='0B68520':\n",
    "        #skip this slide, it is not in the annotated path\n",
    "        continue\n",
    "\n",
    "    #if result already exists, skip\n",
    "    if os.path.exists(os.path.join(data_path, slide, f'{slide}_trad_OD.parquet.gz')):\n",
    "        print(f\"Results for {slide} already exist, skipping\")\n",
    "        continue\n",
    "\n",
    "    features1 = pd.read_parquet(os.path.join(annotated_path, f'{slide}_trad_ann.parquet.gz'))\n",
    "\n",
    "\n",
    "    features1 = features1.dropna()\n",
    "    features1 = features1.reset_index(drop=True)\n",
    "\n",
    "    #keep all but the last three columns\n",
    "    features = features1.iloc[:, :-3]\n",
    "\n",
    "    print(\"Scaling Features\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    features = scaler.fit_transform(features)\n",
    "    \n",
    "    contamination = 1e-3\n",
    "\n",
    "    print(\"Running COPOD on features\")\n",
    "\n",
    "    clf_name = 'COPOD'\n",
    "    clf = COPOD(n_jobs=-1, contamination=contamination)\n",
    "    clf.fit(features)\n",
    "\n",
    "    print(\"COPOD Completed, Storing results\")\n",
    "\n",
    "    features1['copod_scores'] = clf.decision_scores_\n",
    "\n",
    "    print(\"Running ECOD on features\")\n",
    "    clf_name = 'ECOD'\n",
    "    clf = ECOD(contamination=contamination,n_jobs=-1)\n",
    "    clf.fit(features)\n",
    "    print(\"ECOD Completed, Storing results\")\n",
    "    features1['ecod_scores'] = clf.decision_scores_\n",
    "\n",
    "    print(\"Running IForest on features\")\n",
    "    clf_name = 'IForest'\n",
    "    clf = IForest(contamination=contamination, n_jobs=-1, n_estimators=100)\n",
    "    clf.fit(features)\n",
    "    print(\"IForest Completed, Storing results\")\n",
    "    features1['iforest_scores'] = clf.decision_scores_\n",
    "\n",
    "    #write the features1 to a parquet file\n",
    "    features1 = features1.reset_index(drop=True)\n",
    "    features1.to_parquet(os.path.join(data_path, slide, f'{slide}_trad_OD.parquet.gz'), index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0B68620\n",
      "Results for 0B68620 already exist, skipping\n",
      "Processing 0B60414\n",
      "Results for 0B60414 already exist, skipping\n",
      "Processing 0B87816\n",
      "Results for 0B87816 already exist, skipping\n",
      "Processing 0B87911\n",
      "Results for 0B87911 already exist, skipping\n",
      "Processing 0B68720\n",
      "Results for 0B68720 already exist, skipping\n",
      "Processing 0B8B306\n",
      "Processing 0B68520\n",
      "Processing 0B60214\n",
      "Results for 0B60214 already exist, skipping\n",
      "Processing 0B60314\n",
      "Scaling Features\n",
      "Running COPOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:   21.1s remaining: 10.9min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   36.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPOD Completed, Storing results\n",
      "Running ECOD on features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=64)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=64)]: Done   2 out of  64 | elapsed:    3.2s remaining:  1.6min\n",
      "[Parallel(n_jobs=64)]: Done  64 out of  64 | elapsed:   18.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECOD Completed, Storing results\n",
      "Running IForest on features\n",
      "IForest Completed, Storing results\n",
      "Processing 0B87711\n",
      "Results for 0B87711 already exist, skipping\n",
      "Processing 0B60114\n",
      "Processing 0B68820\n",
      "Results for 0B68820 already exist, skipping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#add PCA of the features to the features1 dataframe\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data_path = '/mnt/deepstore/Final_DeepPhenotyping/pipeline/output/spike_0428'\n",
    "annotated_path = '/mnt/deepstore/Final_DeepPhenotyping/figures/figure5_outlierdetection/trad_features'\n",
    "slides = os.listdir(data_path)\n",
    "slides = [s for s in slides if os.path.isdir(os.path.join(data_path, s))]\n",
    "\n",
    "#make a dictionary to store the results for each slide\n",
    "results = {}\n",
    "\n",
    "\n",
    "for slide in slides: \n",
    "\n",
    "    print(f'Processing {slide}')\n",
    "\n",
    "    if slide=='0B8B306':\n",
    "        continue\n",
    "    elif slide=='0B60114':\n",
    "        #skip this slide, it is not in the annotated path\n",
    "        continue\n",
    "    elif slide=='0B68520':\n",
    "        #skip this slide, it is not in the annotated path\n",
    "        continue\n",
    "\n",
    "    #if result already exists, skip\n",
    "    if os.path.exists(os.path.join(data_path, slide, f'{slide}_trad_PCA_OD.parquet.gz')):\n",
    "        print(f\"Results for {slide} already exist, skipping\")\n",
    "        continue\n",
    "\n",
    "    features1 = pd.read_parquet(os.path.join(annotated_path, f'{slide}_trad_ann.parquet.gz'))\n",
    "\n",
    "\n",
    "    features1 = features1.dropna()\n",
    "    features1 = features1.reset_index(drop=True)\n",
    "\n",
    "    #keep all but the last three columns\n",
    "    features = features1.iloc[:, :-3]\n",
    "\n",
    "    print(\"Scaling Features\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    features = scaler.fit_transform(features)\n",
    "\n",
    "    #run PCA on the features\n",
    "    pca = PCA(n_components=128)\n",
    "    features = pca.fit_transform(features)\n",
    "    \n",
    "    contamination = 1e-3\n",
    "\n",
    "    print(\"Running COPOD on features\")\n",
    "\n",
    "    clf_name = 'COPOD'\n",
    "    clf = COPOD(n_jobs=-1, contamination=contamination)\n",
    "    clf.fit(features)\n",
    "\n",
    "    print(\"COPOD Completed, Storing results\")\n",
    "\n",
    "    features1['copod_scores'] = clf.decision_scores_\n",
    "\n",
    "    print(\"Running ECOD on features\")\n",
    "    clf_name = 'ECOD'\n",
    "    clf = ECOD(contamination=contamination,n_jobs=-1)\n",
    "    clf.fit(features)\n",
    "    print(\"ECOD Completed, Storing results\")\n",
    "    features1['ecod_scores'] = clf.decision_scores_\n",
    "\n",
    "    print(\"Running IForest on features\")\n",
    "    clf_name = 'IForest'\n",
    "    clf = IForest(contamination=contamination, n_jobs=-1, n_estimators=100)\n",
    "    clf.fit(features)\n",
    "    print(\"IForest Completed, Storing results\")\n",
    "    features1['iforest_scores'] = clf.decision_scores_\n",
    "\n",
    "    #write the features1 to a parquet file\n",
    "    features1 = features1.reset_index(drop=True)\n",
    "    features1.to_parquet(os.path.join(data_path, slide, f'{slide}_trad_PCA_OD.parquet.gz'), index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#add PCA of the features to the features1 dataframe\n",
    "\n",
    "from sklearn.random_project import GaussianRandomProjection\n",
    "\n",
    "data_path = '/mnt/deepstore/Final_DeepPhenotyping/pipeline/output/spike_0428'\n",
    "annotated_path = '/mnt/deepstore/Final_DeepPhenotyping/figures/figure5_outlierdetection/trad_features'\n",
    "slides = os.listdir(data_path)\n",
    "slides = [s for s in slides if os.path.isdir(os.path.join(data_path, s))]\n",
    "\n",
    "#make a dictionary to store the results for each slide\n",
    "results = {}\n",
    "\n",
    "\n",
    "for slide in slides: \n",
    "\n",
    "    print(f'Processing {slide}')\n",
    "\n",
    "    if slide=='0B8B306':\n",
    "        continue\n",
    "    elif slide=='0B60114':\n",
    "        #skip this slide, it is not in the annotated path\n",
    "        continue\n",
    "    elif slide=='0B68520':\n",
    "        #skip this slide, it is not in the annotated path\n",
    "        continue\n",
    "\n",
    "    #if result already exists, skip\n",
    "    if os.path.exists(os.path.join(data_path, slide, f'{slide}_trad_JL_OD.parquet.gz')):\n",
    "        print(f\"Results for {slide} already exist, skipping\")\n",
    "        continue\n",
    "\n",
    "    features1 = pd.read_parquet(os.path.join(annotated_path, f'{slide}_trad_ann.parquet.gz'))\n",
    "\n",
    "\n",
    "    features1 = features1.dropna()\n",
    "    features1 = features1.reset_index(drop=True)\n",
    "\n",
    "    #keep all but the last three columns\n",
    "    features = features1.iloc[:, :-3]\n",
    "\n",
    "    print(\"Scaling Features\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    features = scaler.fit_transform(features)\n",
    "\n",
    "    #run PCA on the features\n",
    "    rp = GaussianRandomProjection(n_components=128)\n",
    "    features = rp.fit_transform(features)\n",
    "    \n",
    "    contamination = 1e-3\n",
    "\n",
    "    print(\"Running COPOD on features\")\n",
    "\n",
    "    clf_name = 'COPOD'\n",
    "    clf = COPOD(n_jobs=-1, contamination=contamination)\n",
    "    clf.fit(features)\n",
    "\n",
    "    print(\"COPOD Completed, Storing results\")\n",
    "\n",
    "    features1['copod_scores'] = clf.decision_scores_\n",
    "\n",
    "    print(\"Running ECOD on features\")\n",
    "    clf_name = 'ECOD'\n",
    "    clf = ECOD(contamination=contamination,n_jobs=-1)\n",
    "    clf.fit(features)\n",
    "    print(\"ECOD Completed, Storing results\")\n",
    "    features1['ecod_scores'] = clf.decision_scores_\n",
    "\n",
    "    print(\"Running IForest on features\")\n",
    "    clf_name = 'IForest'\n",
    "    clf = IForest(contamination=contamination, n_jobs=-1, n_estimators=100)\n",
    "    clf.fit(features)\n",
    "    print(\"IForest Completed, Storing results\")\n",
    "    features1['iforest_scores'] = clf.decision_scores_\n",
    "\n",
    "    #write the features1 to a parquet file\n",
    "    features1 = features1.reset_index(drop=True)\n",
    "    features1.to_parquet(os.path.join(data_path, slide, f'{slide}_trad_JL_OD.parquet.gz'), index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prism",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
