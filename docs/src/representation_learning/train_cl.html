<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>src.representation_learning.train_cl API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.representation_learning.train_cl</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.representation_learning.train_cl.Trainer"><code class="flex name class">
<span>class <span class="ident">Trainer</span></span>
<span>(</span><span>config)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Trainer(object):
    &#34;&#34;&#34;
    Trainer class to manage the training, validation, and model saving process.
    Uses WandB for experiment tracking and logging.

    Attributes:
        main_config (dict): Configuration dictionary containing training settings.
        best_pred (float): Best validation loss observed during training.
        best_epoch (int): Epoch at which the best validation loss occurred.
        model (torch.nn.Module): The neural network model to be trained.
        optimizer (torch.optim.Optimizer): Optimizer for training.
        scheduler (torch.optim.lr_scheduler): Learning rate scheduler.
        train_loader (DataLoader): DataLoader for the training set.
        val_loader (DataLoader): DataLoader for the validation set.

    Methods:
        run_training(): Executes the entire training pipeline.
        build_dataset(config): Loads the training and validation data.
        build_optimizer(config): Sets up the optimizer based on configuration.
        build_scheduler(config): Configures the learning rate scheduler.
        build_model(config): Instantiates the model to be trained.
        train(config): Trains the model for the specified number of epochs.
        validate(temp): Evaluates the model on the validation dataset.
        save_checkpoint(model, epoch, dir): Saves the model checkpoint.
    &#34;&#34;&#34;

    def __init__(self, config):
        &#34;&#34;&#34;
        Initializes the Trainer object.

        Parameters:
            config (dict): Configuration dictionary with model and training parameters.
        &#34;&#34;&#34;
        self.main_config = config

    def run_training(self):
        &#34;&#34;&#34;
        Runs the complete training process including dataset loading, model initialization,
        optimizer configuration, scheduler setup, and model training.
        &#34;&#34;&#34;
        with wandb.init():
            # Set random seed for reproducibility
            if self.main_config[&#39;random_seed&#39;] is not None:
                np.random.seed(self.main_config[&#39;random_seed&#39;])
                torch.manual_seed(self.main_config[&#39;random_seed&#39;])
                torch.cuda.manual_seed(self.main_config[&#39;random_seed&#39;])

            self.best_pred = np.inf  # Initialize best loss to infinity
            self.best_epoch = 0  # Track the best epoch

            # Build the dataset, model, optimizer, and scheduler
            self.build_dataset(config=wandb.config)
            self.build_model(config=wandb.config)
            self.build_optimizer(config=wandb.config)
            self.build_scheduler(config=wandb.config)

            # Start training
            self.train(config=wandb.config)

    def build_dataset(self, config):
        &#34;&#34;&#34;
        Builds the training and validation datasets.

        Parameters:
            config (dict): Configuration dictionary from WandB.
        &#34;&#34;&#34;
        print(config)
        self.train_loader, self.val_loader = get_data_loaders(
            data_path=self.main_config[&#39;data_path&#39;],
            batch_size=config.batch_size
        )

    def build_optimizer(self, config):
        &#34;&#34;&#34;
        Initializes the optimizer based on the configuration.

        Parameters:
            config (dict): Configuration dictionary specifying optimizer type and parameters.
        &#34;&#34;&#34;
        if config.optimizer == &#34;sgd&#34;:
            self.optimizer = torch.optim.SGD(
                params=self.model.parameters(),
                lr=config.lr,
                weight_decay=config.weight_decay,
                momentum=config.momentum
            )
        elif config.optimizer == &#34;adam&#34;:
            self.optimizer = torch.optim.Adam(
                params=self.model.parameters(),
                lr=config.max_lr,
                weight_decay=config.weight_decay
            )

    def build_scheduler(self, config):
        &#34;&#34;&#34;
        Sets up the learning rate scheduler based on the specified type.

        Parameters:
            config (dict): Configuration dictionary with scheduler settings.
        &#34;&#34;&#34;
        # Scheduler with linear warmup and exponential decay
        if config.scheduler == &#39;LambdaLR&#39;:
            lr_multiplier = config.max_lr / config.base_lr
            self.scheduler = LambdaLR(
                optimizer=self.optimizer,
                lr_lambda=lambda epoch: (
                    ((lr_multiplier - 1) * epoch / config.l_e + 1) 
                    if epoch &lt; config.l_e 
                    else lr_multiplier * (config.l_b ** (epoch - config.l_e))
                )
            )
        elif config.scheduler == &#34;Cyclic&#34;:
            self.scheduler = torch.optim.lr_scheduler.CyclicLR(
                optimizer=self.optimizer,
                base_lr=config.base_lr,
                max_lr=config.max_lr,
                step_size_up=72,
                mode=&#39;exp_range&#39;,
                gamma=0.96,
                scale_mode=&#39;cycle&#39;,
                cycle_momentum=False
            )

    def build_model(self, config):
        &#34;&#34;&#34;
        Instantiates the model based on the configuration.

        Parameters:
            config (dict): Configuration dictionary with model parameters.
        &#34;&#34;&#34;
        self.model = Model(
            in_channels=config.in_channels,
            h_dim=config.h_dim,
            projection_dim=config.projection_dim
        ).to(self.main_config[&#39;device&#39;])

    def train(self, config):
        &#34;&#34;&#34;
        Trains the model for a specified number of epochs.

        Parameters:
            config (dict): Configuration dictionary specifying training parameters.
        &#34;&#34;&#34;
        for epoch in range(config.epochs):
            self.model.train()
            train_loss = 0
            for batch_idx, (data, _) in enumerate(self.train_loader):
                data = data.to(self.main_config[&#39;device&#39;])
                self.optimizer.zero_grad()

                # Forward pass and loss calculation
                z_i, z_j, _, _ = self.model(data)
                loss = self.model.loss(z_i, z_j, config.temperature)
                loss.backward()
                self.optimizer.step()

                train_loss += loss.item()
                data = data.detach().cpu()  # Free memory

            # Update scheduler and log training loss
            self.scheduler.step()
            train_loss /= len(self.train_loader)
            print(f&#34;Epoch {epoch} Loss: {train_loss}&#34;, end=&#34;\t&#34;)
            wandb.log({&#34;train_loss&#34;: train_loss})

            # Early stop if loss becomes NaN
            if np.isnan(train_loss):
                print(&#34;Training loss is NaN, stopping.&#34;)
                break

            # Validate the model
            val_loss, _ = self.validate(config.temperature)
            wandb.log({&#34;val_loss&#34;: val_loss})

            # Save the best model
            if epoch == 49:
                self.best_pred = val_loss
                self.best_epoch = epoch
                self.save_checkpoint(self.model, epoch, self.main_config[&#39;model_path&#39;])

    def validate(self, temp):
        &#34;&#34;&#34;
        Validates the model on the validation dataset.

        Parameters:
            temp (float): Temperature parameter for the contrastive loss.

        Returns:
            Tuple[float, list]: Validation loss and list of latent representations.
        &#34;&#34;&#34;
        self.model.eval()
        val_loss = 0
        h_i_list = []

        for batch_idx, (data, label) in enumerate(self.val_loader):
            with torch.no_grad():
                data = data.to(self.main_config[&#39;device&#39;])
                z_i, z_j, _, _ = self.model(data)
                loss = self.model.loss(z_i, z_j, temp)
                val_loss += loss.item()

                # Extract latent representations
                h_i = self.model.encoder(data).detach().cpu().numpy()
                label = label.cpu().numpy()
                for i in range(h_i.shape[0]):
                    h_data = {str(j): h_i[i][j] for j in range(self.model.h_dim)}
                    h_data[&#34;label&#34;] = str(int(label[i]))
                    h_i_list.append(h_data)
                data = data.detach().cpu()

        val_loss /= len(self.val_loader)
        print(f&#34;Validation Loss: {val_loss}&#34;)
        return val_loss, h_i_list

    def save_checkpoint(self, model, epoch, dir):
        &#34;&#34;&#34;
        Saves the model checkpoint.

        Parameters:
            model (torch.nn.Module): The model to save.
            epoch (int): The current epoch number.
            dir (str): Directory to save the checkpoint file.
        &#34;&#34;&#34;
        checkpoint = {
            &#39;epoch&#39;: epoch,
            &#39;model_state_dict&#39;: model.state_dict(),
            &#39;optimizer_state_dict&#39;: self.optimizer.state_dict()
        }
        fname = os.path.join(dir, f&#34;model_ep_{epoch}_loss_{self.best_pred:.4f}.pth&#34;)
        torch.save(checkpoint, fname)</code></pre>
</details>
<div class="desc"><p>Trainer class to manage the training, validation, and model saving process.
Uses WandB for experiment tracking and logging.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>main_config</code></strong> :&ensp;<code>dict</code></dt>
<dd>Configuration dictionary containing training settings.</dd>
<dt><strong><code>best_pred</code></strong> :&ensp;<code>float</code></dt>
<dd>Best validation loss observed during training.</dd>
<dt><strong><code>best_epoch</code></strong> :&ensp;<code>int</code></dt>
<dd>Epoch at which the best validation loss occurred.</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>The neural network model to be trained.</dd>
<dt><strong><code>optimizer</code></strong> :&ensp;<code>torch.optim.Optimizer</code></dt>
<dd>Optimizer for training.</dd>
<dt><strong><code>scheduler</code></strong> :&ensp;<code>torch.optim.lr_scheduler</code></dt>
<dd>Learning rate scheduler.</dd>
<dt><strong><code>train_loader</code></strong> :&ensp;<code>DataLoader</code></dt>
<dd>DataLoader for the training set.</dd>
<dt><strong><code>val_loader</code></strong> :&ensp;<code>DataLoader</code></dt>
<dd>DataLoader for the validation set.</dd>
</dl>
<h2 id="methods">Methods</h2>
<p>run_training(): Executes the entire training pipeline.
build_dataset(config): Loads the training and validation data.
build_optimizer(config): Sets up the optimizer based on configuration.
build_scheduler(config): Configures the learning rate scheduler.
build_model(config): Instantiates the model to be trained.
train(config): Trains the model for the specified number of epochs.
validate(temp): Evaluates the model on the validation dataset.
save_checkpoint(model, epoch, dir): Saves the model checkpoint.</p>
<p>Initializes the Trainer object.</p>
<h2 id="parameters">Parameters</h2>
<p>config (dict): Configuration dictionary with model and training parameters.</p></div>
<h3>Methods</h3>
<dl>
<dt id="src.representation_learning.train_cl.Trainer.build_dataset"><code class="name flex">
<span>def <span class="ident">build_dataset</span></span>(<span>self, config)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_dataset(self, config):
    &#34;&#34;&#34;
    Builds the training and validation datasets.

    Parameters:
        config (dict): Configuration dictionary from WandB.
    &#34;&#34;&#34;
    print(config)
    self.train_loader, self.val_loader = get_data_loaders(
        data_path=self.main_config[&#39;data_path&#39;],
        batch_size=config.batch_size
    )</code></pre>
</details>
<div class="desc"><p>Builds the training and validation datasets.</p>
<h2 id="parameters">Parameters</h2>
<p>config (dict): Configuration dictionary from WandB.</p></div>
</dd>
<dt id="src.representation_learning.train_cl.Trainer.build_model"><code class="name flex">
<span>def <span class="ident">build_model</span></span>(<span>self, config)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_model(self, config):
    &#34;&#34;&#34;
    Instantiates the model based on the configuration.

    Parameters:
        config (dict): Configuration dictionary with model parameters.
    &#34;&#34;&#34;
    self.model = Model(
        in_channels=config.in_channels,
        h_dim=config.h_dim,
        projection_dim=config.projection_dim
    ).to(self.main_config[&#39;device&#39;])</code></pre>
</details>
<div class="desc"><p>Instantiates the model based on the configuration.</p>
<h2 id="parameters">Parameters</h2>
<p>config (dict): Configuration dictionary with model parameters.</p></div>
</dd>
<dt id="src.representation_learning.train_cl.Trainer.build_optimizer"><code class="name flex">
<span>def <span class="ident">build_optimizer</span></span>(<span>self, config)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_optimizer(self, config):
    &#34;&#34;&#34;
    Initializes the optimizer based on the configuration.

    Parameters:
        config (dict): Configuration dictionary specifying optimizer type and parameters.
    &#34;&#34;&#34;
    if config.optimizer == &#34;sgd&#34;:
        self.optimizer = torch.optim.SGD(
            params=self.model.parameters(),
            lr=config.lr,
            weight_decay=config.weight_decay,
            momentum=config.momentum
        )
    elif config.optimizer == &#34;adam&#34;:
        self.optimizer = torch.optim.Adam(
            params=self.model.parameters(),
            lr=config.max_lr,
            weight_decay=config.weight_decay
        )</code></pre>
</details>
<div class="desc"><p>Initializes the optimizer based on the configuration.</p>
<h2 id="parameters">Parameters</h2>
<p>config (dict): Configuration dictionary specifying optimizer type and parameters.</p></div>
</dd>
<dt id="src.representation_learning.train_cl.Trainer.build_scheduler"><code class="name flex">
<span>def <span class="ident">build_scheduler</span></span>(<span>self, config)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_scheduler(self, config):
    &#34;&#34;&#34;
    Sets up the learning rate scheduler based on the specified type.

    Parameters:
        config (dict): Configuration dictionary with scheduler settings.
    &#34;&#34;&#34;
    # Scheduler with linear warmup and exponential decay
    if config.scheduler == &#39;LambdaLR&#39;:
        lr_multiplier = config.max_lr / config.base_lr
        self.scheduler = LambdaLR(
            optimizer=self.optimizer,
            lr_lambda=lambda epoch: (
                ((lr_multiplier - 1) * epoch / config.l_e + 1) 
                if epoch &lt; config.l_e 
                else lr_multiplier * (config.l_b ** (epoch - config.l_e))
            )
        )
    elif config.scheduler == &#34;Cyclic&#34;:
        self.scheduler = torch.optim.lr_scheduler.CyclicLR(
            optimizer=self.optimizer,
            base_lr=config.base_lr,
            max_lr=config.max_lr,
            step_size_up=72,
            mode=&#39;exp_range&#39;,
            gamma=0.96,
            scale_mode=&#39;cycle&#39;,
            cycle_momentum=False
        )</code></pre>
</details>
<div class="desc"><p>Sets up the learning rate scheduler based on the specified type.</p>
<h2 id="parameters">Parameters</h2>
<p>config (dict): Configuration dictionary with scheduler settings.</p></div>
</dd>
<dt id="src.representation_learning.train_cl.Trainer.run_training"><code class="name flex">
<span>def <span class="ident">run_training</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_training(self):
    &#34;&#34;&#34;
    Runs the complete training process including dataset loading, model initialization,
    optimizer configuration, scheduler setup, and model training.
    &#34;&#34;&#34;
    with wandb.init():
        # Set random seed for reproducibility
        if self.main_config[&#39;random_seed&#39;] is not None:
            np.random.seed(self.main_config[&#39;random_seed&#39;])
            torch.manual_seed(self.main_config[&#39;random_seed&#39;])
            torch.cuda.manual_seed(self.main_config[&#39;random_seed&#39;])

        self.best_pred = np.inf  # Initialize best loss to infinity
        self.best_epoch = 0  # Track the best epoch

        # Build the dataset, model, optimizer, and scheduler
        self.build_dataset(config=wandb.config)
        self.build_model(config=wandb.config)
        self.build_optimizer(config=wandb.config)
        self.build_scheduler(config=wandb.config)

        # Start training
        self.train(config=wandb.config)</code></pre>
</details>
<div class="desc"><p>Runs the complete training process including dataset loading, model initialization,
optimizer configuration, scheduler setup, and model training.</p></div>
</dd>
<dt id="src.representation_learning.train_cl.Trainer.save_checkpoint"><code class="name flex">
<span>def <span class="ident">save_checkpoint</span></span>(<span>self, model, epoch, dir)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_checkpoint(self, model, epoch, dir):
    &#34;&#34;&#34;
    Saves the model checkpoint.

    Parameters:
        model (torch.nn.Module): The model to save.
        epoch (int): The current epoch number.
        dir (str): Directory to save the checkpoint file.
    &#34;&#34;&#34;
    checkpoint = {
        &#39;epoch&#39;: epoch,
        &#39;model_state_dict&#39;: model.state_dict(),
        &#39;optimizer_state_dict&#39;: self.optimizer.state_dict()
    }
    fname = os.path.join(dir, f&#34;model_ep_{epoch}_loss_{self.best_pred:.4f}.pth&#34;)
    torch.save(checkpoint, fname)</code></pre>
</details>
<div class="desc"><p>Saves the model checkpoint.</p>
<h2 id="parameters">Parameters</h2>
<p>model (torch.nn.Module): The model to save.
epoch (int): The current epoch number.
dir (str): Directory to save the checkpoint file.</p></div>
</dd>
<dt id="src.representation_learning.train_cl.Trainer.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, config)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, config):
    &#34;&#34;&#34;
    Trains the model for a specified number of epochs.

    Parameters:
        config (dict): Configuration dictionary specifying training parameters.
    &#34;&#34;&#34;
    for epoch in range(config.epochs):
        self.model.train()
        train_loss = 0
        for batch_idx, (data, _) in enumerate(self.train_loader):
            data = data.to(self.main_config[&#39;device&#39;])
            self.optimizer.zero_grad()

            # Forward pass and loss calculation
            z_i, z_j, _, _ = self.model(data)
            loss = self.model.loss(z_i, z_j, config.temperature)
            loss.backward()
            self.optimizer.step()

            train_loss += loss.item()
            data = data.detach().cpu()  # Free memory

        # Update scheduler and log training loss
        self.scheduler.step()
        train_loss /= len(self.train_loader)
        print(f&#34;Epoch {epoch} Loss: {train_loss}&#34;, end=&#34;\t&#34;)
        wandb.log({&#34;train_loss&#34;: train_loss})

        # Early stop if loss becomes NaN
        if np.isnan(train_loss):
            print(&#34;Training loss is NaN, stopping.&#34;)
            break

        # Validate the model
        val_loss, _ = self.validate(config.temperature)
        wandb.log({&#34;val_loss&#34;: val_loss})

        # Save the best model
        if epoch == 49:
            self.best_pred = val_loss
            self.best_epoch = epoch
            self.save_checkpoint(self.model, epoch, self.main_config[&#39;model_path&#39;])</code></pre>
</details>
<div class="desc"><p>Trains the model for a specified number of epochs.</p>
<h2 id="parameters">Parameters</h2>
<p>config (dict): Configuration dictionary specifying training parameters.</p></div>
</dd>
<dt id="src.representation_learning.train_cl.Trainer.validate"><code class="name flex">
<span>def <span class="ident">validate</span></span>(<span>self, temp)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validate(self, temp):
    &#34;&#34;&#34;
    Validates the model on the validation dataset.

    Parameters:
        temp (float): Temperature parameter for the contrastive loss.

    Returns:
        Tuple[float, list]: Validation loss and list of latent representations.
    &#34;&#34;&#34;
    self.model.eval()
    val_loss = 0
    h_i_list = []

    for batch_idx, (data, label) in enumerate(self.val_loader):
        with torch.no_grad():
            data = data.to(self.main_config[&#39;device&#39;])
            z_i, z_j, _, _ = self.model(data)
            loss = self.model.loss(z_i, z_j, temp)
            val_loss += loss.item()

            # Extract latent representations
            h_i = self.model.encoder(data).detach().cpu().numpy()
            label = label.cpu().numpy()
            for i in range(h_i.shape[0]):
                h_data = {str(j): h_i[i][j] for j in range(self.model.h_dim)}
                h_data[&#34;label&#34;] = str(int(label[i]))
                h_i_list.append(h_data)
            data = data.detach().cpu()

    val_loss /= len(self.val_loader)
    print(f&#34;Validation Loss: {val_loss}&#34;)
    return val_loss, h_i_list</code></pre>
</details>
<div class="desc"><p>Validates the model on the validation dataset.</p>
<h2 id="parameters">Parameters</h2>
<p>temp (float): Temperature parameter for the contrastive loss.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple[float, list]</code></dt>
<dd>Validation loss and list of latent representations.</dd>
</dl></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.representation_learning" href="index.html">src.representation_learning</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.representation_learning.train_cl.Trainer" href="#src.representation_learning.train_cl.Trainer">Trainer</a></code></h4>
<ul class="two-column">
<li><code><a title="src.representation_learning.train_cl.Trainer.build_dataset" href="#src.representation_learning.train_cl.Trainer.build_dataset">build_dataset</a></code></li>
<li><code><a title="src.representation_learning.train_cl.Trainer.build_model" href="#src.representation_learning.train_cl.Trainer.build_model">build_model</a></code></li>
<li><code><a title="src.representation_learning.train_cl.Trainer.build_optimizer" href="#src.representation_learning.train_cl.Trainer.build_optimizer">build_optimizer</a></code></li>
<li><code><a title="src.representation_learning.train_cl.Trainer.build_scheduler" href="#src.representation_learning.train_cl.Trainer.build_scheduler">build_scheduler</a></code></li>
<li><code><a title="src.representation_learning.train_cl.Trainer.run_training" href="#src.representation_learning.train_cl.Trainer.run_training">run_training</a></code></li>
<li><code><a title="src.representation_learning.train_cl.Trainer.save_checkpoint" href="#src.representation_learning.train_cl.Trainer.save_checkpoint">save_checkpoint</a></code></li>
<li><code><a title="src.representation_learning.train_cl.Trainer.train" href="#src.representation_learning.train_cl.Trainer.train">train</a></code></li>
<li><code><a title="src.representation_learning.train_cl.Trainer.validate" href="#src.representation_learning.train_cl.Trainer.validate">validate</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
