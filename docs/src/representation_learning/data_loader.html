<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>src.representation_learning.data_loader API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.representation_learning.data_loader</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="src.representation_learning.data_loader.get_data_loaders"><code class="name flex">
<span>def <span class="ident">get_data_loaders</span></span>(<span>data_path, batch_size=64)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_data_loaders(data_path, batch_size=64):
    &#34;&#34;&#34;
    Creates training and validation data loaders from HDF5 files containing images and masks.
    The function reads data from subdirectories, splits it into training and validation sets,
    and returns data loaders for both sets.

    Parameters:
        data_path (str): Path to the directory containing subfolders with HDF5 files.
        batch_size (int): Number of samples per batch for the data loaders (default: 64).

    Returns:
        Tuple[DataLoader, DataLoader]: 
            - train_loader: DataLoader object for the training set.
            - val_loader: DataLoader object for the validation set.
    &#34;&#34;&#34;
    # Get subdirectory names (representing classes/individual slides of data) within the data path
    types = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))]

    # Initialize lists to store images, masks, and labels for training and validation
    train_images_list, val_images_list = [], []
    train_masks_list, val_masks_list = [], []
    train_labels_list, val_labels_list = [], []

    print(types)  # Print available types (classes)

    # Iterate over each class/slide folder to load images and masks
    for label, t in enumerate(types):
        print(t)
        current_type_path = os.path.join(data_path, t)

        # Get the list of HDF5 files for the current class/slide
        current_type_files = glob.glob(os.path.join(current_type_path, &#34;*.hdf5&#34;))

        class_images, class_masks = [], []

        # Load images and masks from each HDF5 file
        for file_path in current_type_files:
            with h5py.File(file_path, &#39;r&#39;) as f:
                # Load images and masks, ensuring the correct data types
                imgs = np.array(f[&#39;images&#39;][:], dtype=np.float32)
                msks = np.array(f[&#39;masks&#39;][:])

                # Append to the current class lists
                class_images.append(imgs)
                class_masks.append(msks)

        # Concatenate all loaded images and masks for the current class/slide
        class_images = np.concatenate(class_images, axis=0)
        class_masks = np.concatenate(class_masks, axis=0)

        # Determine the number of training samples (80% of the dataset)
        num_train = int(len(class_images) * 0.8)

        # Randomly shuffle and split data into training and validation sets
        indices = np.random.permutation(len(class_images))
        train_indices = indices[:num_train]
        val_indices = indices[num_train:]

        train_imgs, val_imgs = class_images[train_indices], class_images[val_indices]
        train_masks, val_masks = class_masks[train_indices], class_masks[val_indices]

        # Append the training and validation data to the respective lists
        train_images_list.append(train_imgs)
        train_masks_list.append(train_masks)
        train_labels_list.append(np.full(len(train_imgs), label, dtype=np.int64))

        print(len(train_imgs))  # Print the number of training images loaded

        val_images_list.append(val_imgs)
        val_masks_list.append(val_masks)
        val_labels_list.append(np.full(len(val_imgs), label, dtype=np.int64))

    # Concatenate lists from all classes to create the final training and validation sets
    train_images = np.concatenate(train_images_list, axis=0)
    val_images = np.concatenate(val_images_list, axis=0)
    train_labels = np.concatenate(train_labels_list, axis=0)
    val_labels = np.concatenate(val_labels_list, axis=0)
    train_masks = np.concatenate(train_masks_list, axis=0)
    val_masks = np.concatenate(val_masks_list, axis=0)

    print(len(train_images), len(val_images))  # Print the number of total training and validation images

    # Create PyTorch datasets using the custom dataset class
    train_dataset = CustomImageDataset(train_images, train_masks, train_labels, tran=True)
    val_dataset = CustomImageDataset(val_images, val_masks, val_labels, tran=False)

    # Create DataLoaders with shuffling for training and no shuffling for validation
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)

    return train_loader, val_loader</code></pre>
</details>
<div class="desc"><p>Creates training and validation data loaders from HDF5 files containing images and masks.
The function reads data from subdirectories, splits it into training and validation sets,
and returns data loaders for both sets.</p>
<h2 id="parameters">Parameters</h2>
<p>data_path (str): Path to the directory containing subfolders with HDF5 files.
batch_size (int): Number of samples per batch for the data loaders (default: 64).</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple[DataLoader, DataLoader]</code></dt>
<dd>
<ul>
<li>train_loader: DataLoader object for the training set.</li>
<li>val_loader: DataLoader object for the validation set.</li>
</ul>
</dd>
</dl></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.representation_learning.data_loader.CustomImageDataset"><code class="flex name class">
<span>class <span class="ident">CustomImageDataset</span></span>
<span>(</span><span>images, masks, labels, tran=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CustomImageDataset(Dataset):
    &#34;&#34;&#34;
    Custom dataset class for loading and processing 4-channel, 75x75, 16-bit TIFF images.
    Each sample includes a normalized image, a binary mask, and a label. The dataset
    supports applying transformations during data loading.

    Attributes:
        images (np.ndarray): Numpy array of images with shape (N, 4, 75, 75).
        masks (np.ndarray): Numpy array of binary masks with shape (N, 1, 75, 75).
        labels (np.ndarray): Numpy array of labels corresponding to each image.
        tran (bool): Flag to indicate if transformations should be applied.
        t (torchvision.transforms.Compose): Transformation pipeline to convert numpy arrays to tensors.

    Methods:
        __len__(): Returns the number of samples in the dataset.
        __getitem__(idx): Retrieves the image, mask, and label at the given index.
    &#34;&#34;&#34;

    def __init__(self, images, masks, labels, tran=False):
        &#34;&#34;&#34;
        Initializes the custom image dataset.

        Parameters:
            images (np.ndarray): Array of input images.
            masks (np.ndarray): Array of binary masks corresponding to the images.
            labels (np.ndarray): Array of integer labels for each image.
            tran (bool): If True, applies transformations during data loading.
        &#34;&#34;&#34;
        self.images = images
        self.masks = masks
        self.labels = labels
        self.tran = tran

        # Transformation pipeline: Convert numpy arrays to PyTorch tensors
        self.t = transforms.Compose([
            transforms.ToTensor()
        ])

    def __len__(self):
        &#34;&#34;&#34;
        Returns the number of samples in the dataset.

        Returns:
            int: The number of images in the dataset.
        &#34;&#34;&#34;
        return len(self.images)

    def __getitem__(self, idx):
        &#34;&#34;&#34;
        Retrieves the image, mask, and label at the given index.

        Parameters:
            idx (int): Index of the sample to retrieve.

        Returns:
            Tuple[torch.Tensor, torch.Tensor]:
                - hard_masked_image: The image multiplied by the mask, concatenated with the mask.
                - label: The label corresponding to the image.
        &#34;&#34;&#34;
        # Normalize the image to the range [0, 1]
        image = self.images[idx].astype(np.float32) / 65535.0
        
        # Retrieve the corresponding label and mask
        label = self.labels[idx]
        mask = self.masks[idx].astype(np.int16)

        # Apply transformation to convert to tensor
        image = self.t(image)
        mask = self.t(mask)

        # Create a masked version of the image by multiplying with the binary mask
        hard_masked_image = image * mask

        # Concatenate the masked image and the mask itself along the channel dimension
        hard_masked_image = torch.cat((hard_masked_image, mask), dim=0)

        return hard_masked_image, torch.tensor(label, dtype=torch.long)</code></pre>
</details>
<div class="desc"><p>Custom dataset class for loading and processing 4-channel, 75x75, 16-bit TIFF images.
Each sample includes a normalized image, a binary mask, and a label. The dataset
supports applying transformations during data loading.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>images</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Numpy array of images with shape (N, 4, 75, 75).</dd>
<dt><strong><code>masks</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Numpy array of binary masks with shape (N, 1, 75, 75).</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Numpy array of labels corresponding to each image.</dd>
<dt><strong><code>tran</code></strong> :&ensp;<code>bool</code></dt>
<dd>Flag to indicate if transformations should be applied.</dd>
<dt><strong><code>t</code></strong> :&ensp;<code>torchvision.transforms.Compose</code></dt>
<dd>Transformation pipeline to convert numpy arrays to tensors.</dd>
</dl>
<h2 id="methods">Methods</h2>
<p><strong>len</strong>(): Returns the number of samples in the dataset.
<strong>getitem</strong>(idx): Retrieves the image, mask, and label at the given index.</p>
<p>Initializes the custom image dataset.</p>
<h2 id="parameters">Parameters</h2>
<p>images (np.ndarray): Array of input images.
masks (np.ndarray): Array of binary masks corresponding to the images.
labels (np.ndarray): Array of integer labels for each image.
tran (bool): If True, applies transformations during data loading.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.utils.data.dataset.Dataset</li>
<li>typing.Generic</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.representation_learning" href="index.html">src.representation_learning</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="src.representation_learning.data_loader.get_data_loaders" href="#src.representation_learning.data_loader.get_data_loaders">get_data_loaders</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.representation_learning.data_loader.CustomImageDataset" href="#src.representation_learning.data_loader.CustomImageDataset">CustomImageDataset</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
